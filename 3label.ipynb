{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1hV5E-eGxtP6cEF1rekwfArhuxR5HzQ2B","timestamp":1683025469806}],"authorship_tag":"ABX9TyOWfbUe3PkDUFarD+H12lX4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZWTpcshTnJl","executionInfo":{"status":"ok","timestamp":1683726360734,"user_tz":-330,"elapsed":6,"user":{"displayName":"Shivam","userId":"05864240420023546370"}},"outputId":"95b1836e-1de0-467b-f40d-3c8a887b2543"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Load the dataset\n","url = 'https://raw.githubusercontent.com/synle/machine-learning-sample-dataset/master/liar_dataset/train.tsv'\n","df = pd.read_csv(url, sep='\\t', header=None)\n","\n","# Convert classes to 3\n","mapping = {'pants-fire': 0, 'false': 0, 'barely-true': 1, 'half-true': 2, 'mostly-true': 2, 'true': 2}\n","df[1] = df[1].apply(lambda x: mapping[x])\n","\n","# Tokenize and preprocess the text data\n","stopwords = nltk.corpus.stopwords.words('english')\n","stemmer = nltk.stem.PorterStemmer()\n","\n","def tokenize_and_preprocess(text):\n","    # Tokenize the text\n","    tokens = nltk.word_tokenize(text.lower())\n","\n","    # Remove stop words and stem the remaining words\n","    tokens = [stemmer.stem(token) for token in tokens if token not in stopwords]\n","\n","    # Rejoin the tokens into a single string\n","    return ' '.join(tokens)\n","\n","df[2] = df[2].apply(tokenize_and_preprocess)\n","\n","# Split the dataset into features and labels\n","X = df[2]\n","y = df[1]\n","\n","# Convert the text data to numerical features using TF-IDF vectorization\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(X)\n","\n","# Train a logistic regression model\n","clf = LogisticRegression(max_iter=1000)\n","clf.fit(X, y)\n","\n","# Make predictions on the training set and compute metrics\n","y_pred = clf.predict(X)\n","conf_mat = confusion_matrix(y, y_pred)\n","class_report = classification_report(y, y_pred)\n","\n","# Print the confusion matrix and classification report\n","print('Confusion Matrix:\\n', conf_mat)\n","print('\\nClassification Report:\\n', class_report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6TZO--eW9Jbx","executionInfo":{"status":"ok","timestamp":1683726383507,"user_tz":-330,"elapsed":17360,"user":{"displayName":"Shivam","userId":"05864240420023546370"}},"outputId":"bee6422d-227b-46d0-ec6e-ef28cf54c948"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix:\n"," [[1434   27 1373]\n"," [ 232  251 1171]\n"," [ 208   24 5520]]\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.77      0.51      0.61      2834\n","           1       0.83      0.15      0.26      1654\n","           2       0.68      0.96      0.80      5752\n","\n","    accuracy                           0.70     10240\n","   macro avg       0.76      0.54      0.55     10240\n","weighted avg       0.73      0.70      0.66     10240\n","\n"]}]}]}